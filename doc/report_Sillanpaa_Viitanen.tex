\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{float}
\usepackage{listings}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{pdfpages}
\usepackage{color}
\usepackage[pdftex,colorlinks=true,citecolor=black,
            pagecolor=black,linkcolor=black,menucolor=black,
            urlcolor=black]{hyperref}
\usepackage{eufrak}
\usepackage{amsmath}
\usepackage{amsbsy}
\usepackage{eucal}
%\usepackage{subfigure}
\usepackage{longtable}
\usepackage{url}
\urlstyle{same}

\usepackage{natbib}

\parindent 0mm
\parskip 3mm

\newcommand*\mean[1]{\bar{#1}}

\pdfinfo{            
          /Title      (T-61.5140 Machine Learning: Advanced Probabilistic Methods Project)
          /Author     (Ville Sillanp채채, Lauri Viitanen)
          /Keywords   ()
}

\title{Classify the glass samples using Gaussian Mixture Models (GMM)}
\author{Ville Sillanp채채, k84338 - Lauri Viitanen, 338853 \\ 
       {\it ville.sillanpaa@aalto.fi} -
       {\it lauri.viitanen@aalto.fi}}

\begin{document}
\maketitle
\clearpage

%\begin{figure}[]
%	\includegraphics[]{all_338853_2001.pdf}
%\end{figure}

\section{Introduction}

In this project we analyze a glass data set using both unsupervised and
supervised machine learning approaches. The glass dataset [1] has been
provided by the US Forensic Science Service and is available for download from
the UCI machine learning repository. The dataset consists of samples for 6
types of glass (class label 4 is missing in the current data set); defined in
terms of their oxide content (i.e. Na, Fe, K, etc). In particular, the dataset
contains 214 samples characterized by 9 features and the samples have been
categorized into 6 different classes of glass.

\par
The goal of the project is to classify the glass samples using Gaussian
Mixture Models (GMM) in unsupervised and supervised approaches. In the
unsupervised approach we use GMMs to find clusters from the full data set and
compare the clustering results with the ground-truth class labels that are
provided with the data set. In the supervised approach we fit a separate GMM
for each class label and evaluate the classification performance using the
fitted models on the test set. The dataset has been randomly divided into
training (75 \%) and test sets (25 \%) keeping an equal proportion of different
class instances in the training and test sets. In addition, we compare the
performance of the GMM model with the k-nearest neighbors classifier.

\section{Methodology}

\subsection{Unsupervised Approach}

\subsection{Supervised Approach}

In the supervised approach, the Gaussian mixture model is fitted to every
class label separately. The model complexities range from 2 to 10 components.
Class label 4 has no samples in the training data, class label 6 has only six
samples and class label 5 has nine samples, meaning that using more than nine
classes already drops the number of class labels from seven to four. Only
classes 1 and 2 have more than 21 samples.

\par
The GMM is fitted to the training data using the Expectation-Maximization (EM)
algorithm. The training halts after 200 iterations or if the log likelihood
has not changed enough (0.001 units) during the past few iterations (five).
The resulting fitted Gaussians are then applied to the test data of the same
classes, producing the log likelihood of the test data being produced by the
fitted GMM.

\begin{verbatim}
                  Two clusters
    0.53    0       0.47    0       0       0   
    0.22    0.17    0.50    0       0.11    0   
    0.25    0       0.75    0       0       0   
    0       0.50    0       0.25    0.25    0   
    0       1.00    0       0       0       0   
    0       0       0       0       0.25    0.75

                 Three clusters
    0.65    0       0.35    0       0       0   
    0.33    0.17    0.39    0       0.11    0   
    0.25    0       0.75    0       0       0   
    0       0.50    0       0.25    0.25    0   
    0       0.67    0       0       0.33    0   
    0       0.13    0       0       0.13    0.75

                 Four clusters
    0.65    0       0.35    0       0       0   
    0.39    0.17    0.33    0       0.11    0   
    0.25    0       0.75    0       0       0   
    0       0       0       0.25    0.75    0   
    0       0       0       0       1.00    0   
    0       0.13    0       0       0.13    0.75

                 Five clusters
    0.65    0       0.35    0       0       0   
    0.22    0.17    0.50    0       0.11    0   
    0.25    0       0.75    0       0       0   
    0       0       0       0.25    0.75    0   
    0       0       0       0       1.00    0   
    0       0.13    0       0       0.13    0.75
 
                  Six clusters
    0.65    0       0.35    0       0       0   
    0.22    0.17    0.50    0.11    0       0   
    0.25    0       0.75    0       0       0   
    0       0.50    0       0.50    0       0   
    0       1.00    0       0       0       0   
    0       0.13    0       0.13    0       0.75

                 Seven clusters
    0.65    0       0.35    0       0       0   
    0.22    0.17    0.50    0.11    0       0   
    0.25    0       0.75    0       0       0   
    0       0.50    0       0.50    0       0   
    0       1.00    0       0       0       0   
    0       0.25    0       0       0       0.75

                 Eight clusters
    0.65    0       0.35    0       0       0   
    0.22    0.17    0.50    0.11    0       0   
    0.25    0       0.75    0       0       0   
    0       0.50    0       0.50    0       0   
    0       1.00    0       0       0       0   
    0       0.13    0       0.13    0       0.75

                 Nine clusters
    0.65    0       0.35    0       0       0   
    0.22    0.22    0.50    0       0       0.06
    0.25    0       0.75    0       0       0   
    0       0.75    0       0       0       0.25
    0       1.00    0       0       0       0   
    0       0.13    0       0       0       0.88

                  Ten clusters
    0.65    0       0.35    0       0       0   
    0.22    0.22    0.50    0       0       0.06
    0.25    0       0.75    0       0       0   
    0       0.75    0       0       0       0.25
    0       1.00    0       0       0       0   
    0       0.13    0       0       0       0.88

  Prediction accuracy mean over all cluster counts
    0.63    0       0.37    0       0       0   
    0.25    0.18    0.47    0.04    0.05    0.01
    0.25    0       0.75    0       0       0   
    0       0.44    0       0.28    0.22    0.06
    0       0.74    0       0       0.26    0   
    0       0.13    0       0.03    0.07    0.78

     1 nearest neighbor predicted class accuracy
    0.88    0.06    0.06    0       0       0   
    0.06    0.83    0       0       0.11    0   
    0.50    0       0.50    0       0       0   
    0       0.50    0       0.50    0       0   
    0       0       0       0       1.00    0   
    0.13    0.13    0       0       0       0.75
\end{verbatim}

\par
VISUALIZATION HERE

\par
THEN TALK ABOUT THE MODEL COMPONENT COUNT DISTRIBUTION

\par
VISUALIZATION HERE

\section{Results}

\section{Discussion}

\par
[1] http://archive.ics.uci.edu/ml/datasets/Glass+Identification

%\begin{equation}
%\begin{aligned}
%	r[n] &= h[n] * s[n] \leftrightarrow
%		R(e^{j\omega}) = H(e^{j\omega})S(e^{j\omega}) \\
%	r[n] &= \sum_{k=-\infty}^{\infty} h[k]s[n-k] \\
%\end{aligned}
%\end{equation}

%Figure \ref{fig:100_104} shows combinations of these numbers

%\begin{figure}[H]
%	\includegraphics[scale=0.67]{m100_115_b.eps}
%	\caption{The amplitude response both in linear and logarithmic
%		(decibel) scale.}
%	\label{fig:100_115_b}
%\end{figure}

\clearpage
\appendix

\clearpage
\section*{Appendix A}\label{code:asdf}

Matlab code for asdf.

%\lstinputlisting{asdf.m}

\end{document}
